"""
FastAPI åç«¯ - é˜¶æ®µ1ï¼šåŸºç¡€æ¶æ„
ç›®æ ‡ï¼šå®ç°åŸºæœ¬çš„èŠå¤©åŠŸèƒ½ï¼Œä¸ºåç»­RAGåšå‡†å¤‡
"""

import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv
import google.generativeai as genai

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ– FastAPI
app = FastAPI(
    title="åŒ»å­¦çŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
    description="åŸºäº RAG çš„å‚ç›´é¢†åŸŸé—®ç­”æœºå™¨äºº",
    version="0.1.0"
)

# é…ç½® CORSï¼ˆå…è®¸å‰ç«¯è®¿é—®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Next.js é»˜è®¤ç«¯å£
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
if os.getenv("HTTPS_PROXY"):
    proxy = os.getenv("HTTPS_PROXY")
    print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy}")
    os.environ["HTTP_PROXY"] = proxy
    os.environ["HTTPS_PROXY"] = proxy

# åˆå§‹åŒ– Google AI
api_key = os.getenv("GOOGLE_AI_API_KEY")
if not api_key:
    raise ValueError("âŒ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GOOGLE_AI_API_KEY")

genai.configure(api_key=api_key)

# ==========================================
# æ•°æ®æ¨¡å‹
# ==========================================
class Message(BaseModel):
    """å•æ¡æ¶ˆæ¯"""
    role: str  # "user" æˆ– "assistant"
    content: str

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    messages: List[Message]

# ==========================================
# æ ¸å¿ƒåŠŸèƒ½
# ==========================================

async def generate_stream(messages: List[Message]):
    """
    ç”Ÿæˆæµå¼å“åº”ï¼ˆé˜¶æ®µ1ï¼šç›´æ¥è°ƒç”¨Geminiï¼‰
    """
    try:
        # è·å– Gemini æ¨¡å‹
        model = genai.GenerativeModel('gemini-2.5-flash')

    # å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼ˆæ¨èï¼‰
    # genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
    # å¿«é€Ÿä½†è´¨é‡ç•¥ä½
    #  genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
    # æœ€é«˜è´¨é‡ä½†è¾ƒæ…¢
    # genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });
        
        # æ„å»ºå¯¹è¯å†å²
        chat_history = []
        for msg in messages[:-1]:  # é™¤äº†æœ€åä¸€æ¡
            chat_history.append({
                "role": "user" if msg.role == "user" else "model",
                "parts": [msg.content]
            })
        
        # åˆ›å»ºå¯¹è¯
        chat = model.start_chat(history=chat_history)
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = messages[-1].content
        
        print(f"ğŸ“¨ æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
        
        # æµå¼ç”Ÿæˆ
        response = chat.send_message(user_message, stream=True)
        
        for chunk in response:
            if chunk.text:
                yield chunk.text
                
        print("âœ… å“åº”ç”Ÿæˆå®Œæˆ")
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        yield f"\n\n[é”™è¯¯: {error_msg}]"

# ==========================================
# API è·¯ç”±
# ==========================================

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "message": "åŒ»å­¦çŸ¥è¯†é—®ç­”ç³»ç»Ÿ API",
        "version": "0.1.0",
        "stage": "é˜¶æ®µ1ï¼šåŸºç¡€æ¶æ„"
    }

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """
    èŠå¤©æ¥å£ï¼ˆæµå¼å“åº”ï¼‰
    é˜¶æ®µ1ï¼šç›´æ¥è°ƒç”¨ Geminiï¼Œä¸ä½¿ç”¨ RAG
    """
    try:
        if not request.messages:
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # è¿”å›æµå¼å“åº”
        return StreamingResponse(
            generate_stream(request.messages),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        print(f"âŒ èŠå¤©æ¥å£é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰"""
    return {
        "status": "healthy",
        "google_ai_configured": bool(os.getenv("GOOGLE_AI_API_KEY")),
        "proxy_enabled": bool(os.getenv("HTTPS_PROXY")),
    }

# ==========================================
# å¯åŠ¨æœåŠ¡
# ==========================================

if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("BACKEND_HOST", "0.0.0.0")
    port = int(os.getenv("BACKEND_PORT", 8000))
    
    print(f"""
    ==========================================
    ğŸš€ å¯åŠ¨åŒ»å­¦çŸ¥è¯†é—®ç­”ç³»ç»Ÿåç«¯
    ==========================================
    ğŸ“ åœ°å€: http://{host}:{port}
    ğŸ“– æ–‡æ¡£: http://{host}:{port}/docs
    ğŸ”§ é˜¶æ®µ: é˜¶æ®µ1 - åŸºç¡€æ¶æ„
    ==========================================
    """)
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=True,  # å¼€å‘æ¨¡å¼è‡ªåŠ¨é‡è½½
        log_level="info"
    )