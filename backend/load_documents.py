"""
æ–‡æ¡£åŠ è½½å’Œå‘é‡åŒ–è„šæœ¬ï¼ˆä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ï¼‰
åŠŸèƒ½ï¼šåŠ è½½çŸ¥è¯†åº“æ–‡æ¡£ï¼Œåˆ†å—ï¼Œåˆ›å»ºå‘é‡åµŒå…¥ï¼Œå­˜å…¥ChromaDB
"""

import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

print("=" * 60)
print("ğŸ“š å¼€å§‹åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ï¼‰")
print("=" * 60)

# é…ç½®
DATA_DIR = "./data"
VECTOR_STORE_DIR = "./vector_store"

print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
print(f"ğŸ’¾ å‘é‡åº“ç›®å½•: {VECTOR_STORE_DIR}")

# ==========================================
# æ­¥éª¤1ï¼šåŠ è½½æ–‡æ¡£
# ==========================================
print("\nğŸ” æ­¥éª¤1ï¼šåŠ è½½æ–‡æ¡£...")

# åŠ è½½æ‰€æœ‰txtæ–‡ä»¶
loader = DirectoryLoader(
    DATA_DIR,
    glob="**/*.txt",  # é€’å½’æœç´¢æ‰€æœ‰txtæ–‡ä»¶
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"}
)

try:
    documents = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºåŠ è½½çš„æ–‡æ¡£ä¿¡æ¯
    for i, doc in enumerate(documents, 1):
        file_name = os.path.basename(doc.metadata.get("source", "æœªçŸ¥"))
        content_length = len(doc.page_content)
        print(f"   {i}. {file_name} ({content_length} å­—ç¬¦)")
        
except Exception as e:
    print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
    exit(1)

if len(documents) == 0:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥dataç›®å½•")
    exit(1)

# ==========================================
# æ­¥éª¤2ï¼šæ–‡æ¡£åˆ†å—
# ==========================================
print("\nâœ‚ï¸ æ­¥éª¤2ï¼šæ–‡æ¡£åˆ†å—...")

# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # æ¯å—500å­—ç¬¦
    chunk_overlap=50,      # å—ä¹‹é—´é‡å 50å­—ç¬¦
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
)

# åˆ†å—
try:
    splits = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(splits)} ä¸ªå—")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå—çš„ç¤ºä¾‹
    if len(splits) > 0:
        print(f"\nğŸ“ ç¤ºä¾‹å—ï¼ˆç¬¬1å—ï¼‰ï¼š")
        print(f"æ¥æºï¼š{os.path.basename(splits[0].metadata.get('source', ''))}")
        print(f"å†…å®¹ï¼š{splits[0].page_content[:100]}...")
        
except Exception as e:
    print(f"âŒ æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
    exit(1)

# ==========================================
# æ­¥éª¤3ï¼šåˆ›å»ºå‘é‡åµŒå…¥å¹¶å­˜å…¥æ•°æ®åº“
# ==========================================
print("\nğŸ”¢ æ­¥éª¤3ï¼šåˆ›å»ºå‘é‡åµŒå…¥ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰...")

try:
    # åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹
    print("â³ æ­£åœ¨ä¸‹è½½/åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ï¼Œçº¦400MBï¼‰...")
    print("   æ¨¡å‹ï¼šparaphrase-multilingual-MiniLM-L12-v2")
    print("   ç‰¹ç‚¹ï¼šæ”¯æŒä¸­æ–‡ï¼Œä½“ç§¯å°ï¼Œé€Ÿåº¦å¿«")
    
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},  # ä½¿ç”¨CPUï¼Œå¦‚æœæœ‰GPUå¯æ”¹ä¸º'cuda'
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print("âœ… æœ¬åœ° Embedding æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if os.path.exists(VECTOR_STORE_DIR):
        import shutil
        shutil.rmtree(VECTOR_STORE_DIR)
        print("ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„å‘é‡åº“")
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    print("â³ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=VECTOR_STORE_DIR,
        collection_name="medical_knowledge"
    )
    
    print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
    print(f"   - æ–‡æ¡£æ•°: {len(documents)}")
    print(f"   - åˆ†å—æ•°: {len(splits)}")
    print(f"   - å­˜å‚¨ä½ç½®: {VECTOR_STORE_DIR}")
    print(f"   - ä½¿ç”¨æ¨¡å‹: æœ¬åœ° Sentence Transformers")
    
except Exception as e:
    print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# ==========================================
# æ­¥éª¤4ï¼šæµ‹è¯•æ£€ç´¢
# ==========================================
print("\nğŸ§ª æ­¥éª¤4ï¼šæµ‹è¯•æ£€ç´¢åŠŸèƒ½...")

try:
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "é«˜è¡€å‹æ‚£è€…åº”è¯¥æ³¨æ„ä»€ä¹ˆ"
    print(f"æµ‹è¯•æŸ¥è¯¢ï¼š{test_query}")
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    results = retriever.invoke(test_query)
    
    print(f"âœ… æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£å—ï¼š\n")
    
    for i, doc in enumerate(results, 1):
        file_name = os.path.basename(doc.metadata.get("source", "æœªçŸ¥"))
        print(f"   {i}. æ¥æºï¼š{file_name}")
        print(f"      å†…å®¹ï¼š{doc.page_content[:100]}...\n")
        
except Exception as e:
    print(f"âŒ æµ‹è¯•æ£€ç´¢å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("=" * 60)
print("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
print("=" * 60)
print("\nğŸ’¡ ä¼˜åŠ¿ï¼š")
print("   âœ… å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€API")
print("   âœ… æ— é…é¢é™åˆ¶ï¼Œå¯ä»¥æ— é™ä½¿ç”¨")
print("   âœ… æ”¯æŒä¸­æ–‡ï¼Œæ•ˆæœè‰¯å¥½")
print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python main.py å¯åŠ¨æœåŠ¡")